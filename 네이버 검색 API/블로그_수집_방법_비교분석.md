# 네이버 블로그 콘텐츠 수집 방법 비교 분석

**작성일**: 2026년 1월 31일

---

## 📊 수집 방법 비교 요약

| 방법 | 속도 | 차단 우회 | 구현 난이도 | 비용 | 추천도 |
|------|------|----------|------------|------|--------|
| **1. 모바일 URL + HTTPX** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 무료 | ✅ **최우선 추천** |
| **2. RSS 피드 + requests** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 무료 | ✅ 매우 추천 |
| **3. curl_cffi** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 무료 | ✅ 추천 |
| **4. Firecrawl** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 유료 | 추천 |
| **5. Crawlee** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | 무료 | 추천 |
| **6. Playwright** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 무료 | 최후 수단 |
| **7. Scrapy** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | 무료 | 정적 사이트용 |

---

## 🥇 1순위: 모바일 URL + 경량 HTTP 클라이언트

### 핵심 원리
네이버 블로그는 **데스크톱 URL 크롤링을 차단**하지만, **모바일 URL(m.blog.naver.com)은 상대적으로 접근이 용이**합니다.

### 구현 방법

```python
import httpx
import asyncio
from bs4 import BeautifulSoup

class NaverBlogCrawler:
    """모바일 URL 기반 고속 크롤러"""

    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15",
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "ko-KR,ko;q=0.9"
        }

    def convert_to_mobile_url(self, url: str) -> str:
        """데스크톱 URL을 모바일 URL로 변환"""
        return url.replace("blog.naver.com", "m.blog.naver.com")

    async def fetch_content(self, url: str) -> dict:
        """비동기로 블로그 콘텐츠 추출"""
        mobile_url = self.convert_to_mobile_url(url)

        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(mobile_url, headers=self.headers)
            soup = BeautifulSoup(response.text, "lxml")

            # 본문 추출 (se-main-container 클래스)
            content_div = soup.find("div", class_="se-main-container")

            return {
                "url": url,
                "title": soup.find("meta", property="og:title")["content"] if soup.find("meta", property="og:title") else "",
                "content": content_div.get_text(strip=True) if content_div else "",
                "date": soup.find("span", class_="se_publishDate").text if soup.find("span", class_="se_publishDate") else ""
            }

    async def batch_fetch(self, urls: list, concurrency: int = 10) -> list:
        """병렬 수집 (동시 10개)"""
        semaphore = asyncio.Semaphore(concurrency)

        async def fetch_with_limit(url):
            async with semaphore:
                try:
                    return await self.fetch_content(url)
                except Exception as e:
                    return {"url": url, "error": str(e)}

        tasks = [fetch_with_limit(url) for url in urls]
        return await asyncio.gather(*tasks)

# 사용 예시
async def main():
    crawler = NaverBlogCrawler()
    urls = [
        "https://blog.naver.com/example1/12345",
        "https://blog.naver.com/example2/67890",
    ]
    results = await crawler.batch_fetch(urls)
    print(results)

asyncio.run(main())
```

### 성능 비교
| 지표 | Playwright | HTTPX (모바일) |
|------|------------|---------------|
| 100개 URL 수집 | ~5분 | ~30초 |
| 메모리 사용 | ~500MB | ~50MB |
| CPU 사용 | 높음 | 낮음 |
| 병렬 처리 | 5개 제한 | 50개 이상 가능 |

---

## 🥈 2순위: RSS 피드 활용

### 핵심 원리
네이버 블로그는 각 블로그마다 **RSS 피드**를 제공합니다. 이를 통해 최신 게시글의 메타데이터와 요약을 빠르게 수집할 수 있습니다.

### RSS URL 형식
```
https://rss.blog.naver.com/{블로그ID}.xml
```

### 구현 방법

```python
import feedparser
import httpx
from bs4 import BeautifulSoup

class RSSBlogCrawler:
    """RSS 기반 블로그 크롤러"""

    def get_blog_id_from_url(self, url: str) -> str:
        """URL에서 블로그 ID 추출"""
        # https://blog.naver.com/blogid/12345 -> blogid
        parts = url.replace("https://blog.naver.com/", "").split("/")
        return parts[0]

    def fetch_rss(self, blog_id: str) -> list:
        """RSS 피드에서 게시글 목록 가져오기"""
        rss_url = f"https://rss.blog.naver.com/{blog_id}.xml"
        feed = feedparser.parse(rss_url)

        posts = []
        for entry in feed.entries:
            posts.append({
                "title": entry.title,
                "link": entry.link,
                "published": entry.published,
                "summary": entry.summary,  # 본문 요약
                "author": feed.feed.title
            })

        return posts

    async def fetch_full_content(self, url: str) -> str:
        """개별 게시글 전체 본문 추출"""
        mobile_url = url.replace("blog.naver.com", "m.blog.naver.com")

        async with httpx.AsyncClient() as client:
            response = await client.get(mobile_url)
            soup = BeautifulSoup(response.text, "lxml")

            content = soup.find("div", class_="se-main-container")
            return content.get_text(strip=True) if content else ""

# 사용 예시
crawler = RSSBlogCrawler()
posts = crawler.fetch_rss("soeverygood")
print(f"총 {len(posts)}개 게시글 발견")
for post in posts[:5]:
    print(f"- {post['title']}")
```

### RSS 장점
- ✅ 차단 가능성 거의 없음 (공식 제공)
- ✅ 빠른 속도 (XML 파싱)
- ✅ 최신 20개 게시글 메타데이터 즉시 획득
- ⚠️ 전체 본문은 별도 요청 필요

---

## 🥉 3순위: curl_cffi (TLS 핑거프린트 우회)

### 핵심 원리
**curl_cffi**는 브라우저의 TLS/JA3 핑거프린트를 모방하여 봇 탐지를 우회합니다. requests보다 **30-50% 더 빠르고**, 차단될 확률이 낮습니다.

### 설치
```bash
pip install curl_cffi
```

### 구현 방법

```python
from curl_cffi import requests as curl_requests
from bs4 import BeautifulSoup

class CurlCffiBlogCrawler:
    """curl_cffi 기반 고성능 크롤러"""

    def __init__(self):
        # Chrome 브라우저 핑거프린트 모방
        self.impersonate = "chrome120"

    def fetch_content(self, url: str) -> dict:
        """브라우저 핑거프린트로 콘텐츠 수집"""
        mobile_url = url.replace("blog.naver.com", "m.blog.naver.com")

        response = curl_requests.get(
            mobile_url,
            impersonate=self.impersonate,
            timeout=30
        )

        soup = BeautifulSoup(response.text, "lxml")
        content = soup.find("div", class_="se-main-container")

        return {
            "url": url,
            "content": content.get_text(strip=True) if content else "",
            "status": response.status_code
        }

    def batch_fetch(self, urls: list) -> list:
        """순차 수집 (세션 재사용)"""
        results = []

        with curl_requests.Session(impersonate=self.impersonate) as session:
            for url in urls:
                try:
                    mobile_url = url.replace("blog.naver.com", "m.blog.naver.com")
                    response = session.get(mobile_url, timeout=30)
                    soup = BeautifulSoup(response.text, "lxml")
                    content = soup.find("div", class_="se-main-container")

                    results.append({
                        "url": url,
                        "content": content.get_text(strip=True) if content else ""
                    })
                except Exception as e:
                    results.append({"url": url, "error": str(e)})

        return results

# 사용 예시
crawler = CurlCffiBlogCrawler()
result = crawler.fetch_content("https://blog.naver.com/example/12345")
print(result)
```

### curl_cffi vs requests 비교
| 지표 | requests | curl_cffi |
|------|----------|-----------|
| 속도 | 1x | 1.3-1.5x |
| 차단율 | 높음 | 낮음 |
| HTTP/2 지원 | ❌ | ✅ |
| TLS 우회 | ❌ | ✅ |

---

## 🏆 4순위: Firecrawl (LLM용 최적화)

### 핵심 원리
**Firecrawl**은 웹 페이지를 **LLM에 최적화된 마크다운**으로 변환합니다. HTML 대비 **67% 적은 토큰**을 사용하며, AI 분석에 바로 활용 가능합니다.

### 설치
```bash
pip install firecrawl-py
```

### 구현 방법

```python
from firecrawl import FirecrawlApp

class FirecrawlBlogCrawler:
    """Firecrawl 기반 LLM 최적화 크롤러"""

    def __init__(self, api_key: str):
        self.app = FirecrawlApp(api_key=api_key)

    def scrape_page(self, url: str) -> dict:
        """단일 페이지를 마크다운으로 변환"""
        result = self.app.scrape_url(
            url,
            params={
                "formats": ["markdown", "html"],
                "onlyMainContent": True
            }
        )

        return {
            "url": url,
            "markdown": result.get("markdown", ""),
            "html": result.get("html", ""),
            "metadata": result.get("metadata", {})
        }

    def crawl_site(self, start_url: str, max_pages: int = 100) -> list:
        """사이트 전체 크롤링"""
        result = self.app.crawl_url(
            start_url,
            params={
                "limit": max_pages,
                "scrapeOptions": {
                    "formats": ["markdown"]
                }
            }
        )

        return result.get("data", [])

# 사용 예시
crawler = FirecrawlBlogCrawler(api_key="your_api_key")
result = crawler.scrape_page("https://m.blog.naver.com/example/12345")
print(result["markdown"])
```

### Firecrawl 가격
| 플랜 | 가격 | 크레딧 |
|------|------|--------|
| Free | $0 | 500/월 |
| Starter | $19/월 | 3,000/월 |
| Growth | $99/월 | 50,000/월 |

---

## 🔄 5순위: Crawlee (Python)

### 핵심 원리
**Crawlee**는 Apify에서 만든 오픈소스 크롤링 프레임워크로, **자동 재시도**, **프록시 로테이션**, **세션 관리**를 내장하고 있습니다.

### 설치
```bash
pip install crawlee[all]
```

### 구현 방법

```python
from crawlee.http_crawler import HttpCrawler, HttpCrawlingContext
from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler

async def main():
    crawler = BeautifulSoupCrawler(
        max_requests_per_crawl=100,
        max_concurrency=10,
    )

    @crawler.router.default_handler
    async def request_handler(context: HttpCrawlingContext):
        soup = context.soup

        # 본문 추출
        content = soup.find("div", class_="se-main-container")

        await context.push_data({
            "url": context.request.url,
            "title": soup.title.string if soup.title else "",
            "content": content.get_text(strip=True) if content else ""
        })

    # 크롤링 시작
    await crawler.run([
        "https://m.blog.naver.com/example1/12345",
        "https://m.blog.naver.com/example2/67890",
    ])

# 실행
import asyncio
asyncio.run(main())
```

### Crawlee 장점
- ✅ 자동 재시도 및 에러 핸들링
- ✅ 프록시 자동 로테이션
- ✅ 세션 관리
- ✅ 요청 큐 관리
- ✅ 진행률 추적

---

## 📊 HTTP 클라이언트 성능 비교

### 벤치마크 결과 (1000개 요청)

| 라이브러리 | 동기/비동기 | 평균 속도 | 메모리 | 추천 용도 |
|-----------|------------|----------|--------|----------|
| **AIOHTTP** | 비동기 | ⭐⭐⭐⭐⭐ (최고) | 낮음 | 대규모 병렬 수집 |
| **HTTPX** | 둘 다 | ⭐⭐⭐⭐ | 중간 | 범용 추천 |
| **curl_cffi** | 동기 | ⭐⭐⭐⭐ | 낮음 | 봇 차단 우회 |
| **requests** | 동기 | ⭐⭐ | 중간 | 간단한 작업 |
| **Playwright** | 비동기 | ⭐ | 높음 | JS 렌더링 필수 시 |

### 속도 비교 그래프
```
AIOHTTP     ████████████████████ 100%
HTTPX       ███████████████░░░░░ 75%
curl_cffi   ██████████████░░░░░░ 70%
requests    ████████░░░░░░░░░░░░ 40%
Playwright  ███░░░░░░░░░░░░░░░░░ 15%
```

---

## 🎯 최종 권장 아키텍처

### 하이브리드 접근법

```
                    ┌─────────────────────────┐
                    │     네이버 검색 API      │
                    │   (URL + 메타데이터)     │
                    └───────────┬─────────────┘
                                │
                    ┌───────────▼─────────────┐
                    │      URL 분류기         │
                    └───────────┬─────────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        │                       │                       │
        ▼                       ▼                       ▼
┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│  RSS 피드     │     │ HTTPX/AIOHTTP │     │  Playwright   │
│  (블로거별)   │     │ (모바일 URL)   │     │ (최후 수단)   │
└───────────────┘     └───────────────┘     └───────────────┘
        │                       │                       │
        └───────────────────────┼───────────────────────┘
                                │
                    ┌───────────▼─────────────┐
                    │      콘텐츠 통합        │
                    └───────────┬─────────────┘
                                │
                    ┌───────────▼─────────────┐
                    │      LLM 분석          │
                    └─────────────────────────┘
```

### 구현 전략

```python
class HybridBlogCrawler:
    """하이브리드 블로그 크롤러"""

    def __init__(self):
        self.httpx_crawler = HTTPXCrawler()
        self.rss_crawler = RSSCrawler()
        self.playwright_crawler = None  # 필요시 초기화

    async def crawl(self, urls: list) -> list:
        results = []
        failed_urls = []

        # 1단계: HTTPX로 빠른 수집 시도
        for batch in self.chunk_list(urls, 50):
            batch_results = await self.httpx_crawler.batch_fetch(batch)

            for result in batch_results:
                if result.get("content"):
                    results.append(result)
                else:
                    failed_urls.append(result["url"])

        # 2단계: 실패한 URL은 Playwright로 재시도
        if failed_urls and self.playwright_crawler is None:
            self.playwright_crawler = PlaywrightCrawler()

        if failed_urls:
            playwright_results = await self.playwright_crawler.batch_fetch(failed_urls)
            results.extend(playwright_results)

        return results
```

---

## 📈 성능 최적화 팁

### 1. 연결 풀링
```python
# 세션 재사용으로 TCP 연결 오버헤드 감소
async with httpx.AsyncClient(
    limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
) as client:
    # 여러 요청 처리
```

### 2. 적절한 딜레이
```python
import asyncio

async def rate_limited_fetch(url, delay=0.5):
    await asyncio.sleep(delay)  # 초당 2개 요청
    return await fetch(url)
```

### 3. 프록시 로테이션
```python
proxies = ["http://proxy1:8080", "http://proxy2:8080"]
proxy = random.choice(proxies)
```

### 4. 캐싱
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_content(url):
    return fetch_content(url)
```

---

## 📚 참고 자료

### 공식 문서
- [HTTPX 공식 문서](https://www.python-httpx.org/)
- [curl_cffi GitHub](https://github.com/yifeikong/curl_cffi)
- [Firecrawl 공식 사이트](https://www.firecrawl.dev/)
- [Crawlee Python GitHub](https://github.com/apify/crawlee-python)
- [네이버 검색 API](https://developers.naver.com/)

### 성능 비교 자료
- [HTTPX vs Requests vs AIOHTTP](https://brightdata.com/blog/web-data/requests-vs-httpx-vs-aiohttp)
- [Playwright vs Selenium](https://blog.apify.com/playwright-vs-selenium/)
- [Best Python Headless Browsers](https://scrape.do/blog/python-headless-browser/)

---

## 🏁 결론

**네이버 블로그 수집에 가장 효율적인 방법:**

1. **1순위**: 모바일 URL + HTTPX/AIOHTTP (빠르고 무료)
2. **2순위**: RSS 피드 활용 (차단 없음, 제한적 데이터)
3. **3순위**: curl_cffi (봇 탐지 우회)
4. **최후 수단**: Playwright (JS 렌더링 필수 시)

> 💡 **핵심**: Playwright는 **최후의 수단**으로 두고, 먼저 경량 HTTP 클라이언트로 시도하세요. 대부분의 네이버 블로그는 모바일 URL로 접근하면 수집 가능합니다.
